# Achieving 99.9% Uptime for ML Systems: The Production MLOps Playbook (From 350+ Users)

**Meta Description:** Learn the exact MLOps strategies I use to maintain 99.9% uptime for my ML system serving 350+ users. Covers monitoring, deployment, incident response, and cost optimization.

**Slug:** /mlops-playbook-999-uptime-production-ml-systems

**Reading Time:** 10 min

---

![System monitoring dashboard showing 99.9% uptime](placeholder-uptime-dashboard.png)

## The Hidden Cost of ML Downtime (That No One Talks About)

Here's what happened at 2:47 AM on a Tuesday in July 2024:

My phone explodes with notifications. The ML prediction API is down. 350+ users can't get predictions. My Discord is on fire. My email inbox is filling up faster than I can read.

**Downtime duration:** 23 minutes  
**Users affected:** 350+  
**Revenue lost:** ~$40  
**Trust lost:** Incalculable

The irony? My ML model had **72% accuracy** that week‚Äîthe highest ever. But nobody cares about accuracy when your system is down.

This incident taught me a brutal lesson: **In production ML, reliability > accuracy.**

Fast forward to today, my system has maintained **99.9% uptime** for 12 consecutive months. That's only **43 minutes of total downtime** in an entire year (all planned maintenance).

This post is the complete playbook I wish I had when I started. No fluff. Just battle-tested strategies from keeping a real ML system running 24/7.

---

## The 99.9% Uptime Framework (5 Pillars)

After one year and 350+ production users, I've distilled everything into 5 pillars:

1. **Infrastructure Design:** Build for failure from day 1
2. **Monitoring:** Detect problems before users do
3. **Deployment:** Zero-downtime updates
4. **Incident Response:** Fix fast, learn faster
5. **Cost Optimization:** Reliability doesn't require a massive budget

Let's dive into each one.

---

## Pillar 1: Infrastructure Design for High Availability

**Rule #1: Assume everything will fail. Design accordingly.**

### My High-Availability Architecture

```
                    [Cloudflare CDN]
                          ‚Üì
                [Load Balancer (NGINX)]
                     /         \
        [API Instance 1]   [API Instance 2]
              ‚Üì                   ‚Üì
        [Redis Primary]    [Redis Replica]
              ‚Üì                   ‚Üì
     [PostgreSQL Primary] ‚Üí [PostgreSQL Standby]
```

**Key Design Decisions:**

### 1. Redundant API Instances (No Single Point of Failure)

**Why it matters:** If one API instance crashes, the other handles traffic seamlessly.

**My setup:**
- 2x DigitalOcean Droplets (4GB RAM each)
- NGINX load balancer with health checks
- Automatic failover if instance becomes unhealthy

```nginx
# NGINX load balancer config
upstream api_backend {
    server api1.internal:8000 max_fails=3 fail_timeout=30s;
    server api2.internal:8000 max_fails=3 fail_timeout=30s;
    
    # Health check endpoint
    keepalive 32;
}

server {
    listen 443 ssl http2;
    server_name api.sabiscore.com;
    
    location / {
        proxy_pass http://api_backend;
        proxy_next_upstream error timeout http_500 http_502 http_503;
        proxy_connect_timeout 2s;
        proxy_send_timeout 10s;
        proxy_read_timeout 10s;
    }
    
    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
    }
}
```

**Cost:** $48/month for 2 instances (vs. $24 for 1)  
**Downtime prevented:** Saved me 6x in the past year

### 2. Database Replication (Read Replicas + Standby)

**Why it matters:** Primary database crashes? Standby takes over in under 30 seconds.

**PostgreSQL Streaming Replication Setup:**

```bash
# On primary database
# postgresql.conf
wal_level = replica
max_wal_senders = 3
wal_keep_size = 1GB

# pg_hba.conf (allow replica connections)
host replication replicator standby_ip/32 md5

# On standby database
# recovery.conf
standby_mode = 'on'
primary_conninfo = 'host=primary_ip port=5432 user=replicator password=xxx'
trigger_file = '/tmp/postgresql.trigger.5432'
```

**Failover process (automated):**
1. Health check detects primary DB is down
2. Script promotes standby to primary (creates trigger file)
3. Update connection strings in application
4. Total downtime: under 30 seconds

**I use:** DigitalOcean Managed PostgreSQL (handles this automatically for $15/month)

### 3. Redis for Caching + Session Management

**Why it matters:** 90% of requests hit cache. If Redis goes down, API latency explodes from 87ms to 800ms.

**Redis persistence strategy:**

```conf
# redis.conf
# RDB snapshots (backup every 5 minutes if ‚â•1 key changed)
save 300 1

# AOF (append-only file for durability)
appendonly yes
appendfsync everysec

# Replication for high availability
replicaof redis-primary.internal 6379
replica-read-only yes
```

**Cache invalidation strategy:**

```python
# Smart cache invalidation (prevents stale predictions)
import redis
from datetime import datetime, timedelta

redis_client = redis.Redis(host='localhost', decode_responses=True)

def cache_prediction(game_id, prediction, expiry_minutes=60):
    """Cache prediction with smart TTL based on game time"""
    game_time = get_game_start_time(game_id)
    time_until_game = (game_time - datetime.now()).total_seconds() / 60
    
    # Cache expires 5 minutes before game starts (to account for late news)
    ttl = min(expiry_minutes, max(5, time_until_game - 5)) * 60
    
    redis_client.setex(
        f"prediction:{game_id}",
        int(ttl),
        json.dumps(prediction)
    )
```

**Result:** Cache hit rate of 89% ‚Üí saves database queries + improves latency

### 4. Cloudflare for DDoS Protection + CDN

**Why it matters:** One DDoS attack can take down your site. Cloudflare blocks it before it reaches your servers.

**Free tier includes:**
- Unlimited DDoS protection
- Global CDN for static assets
- SSL/TLS encryption
- Rate limiting (configurable rules)

**Rate limiting rules I use:**

```javascript
// Cloudflare rate limiting rule
// Prevents API abuse (max 100 requests per 10 minutes per IP)

(http.request.uri.path contains "/api/predict") and 
(rate(1m) > 100)

// Action: Block for 30 minutes
```

**Cost:** $0 (free tier is sufficient for my scale)  
**Attacks blocked in past year:** 12 (would've caused downtime without Cloudflare)

---

## Pillar 2: Monitoring That Actually Prevents Failures

**Rule #2: If you're not monitoring it, it will break at 3 AM.**

### My Monitoring Stack (Total Cost: $0/month)

| Tool | Purpose | Cost |
|------|---------|------|
| **UptimeRobot** | HTTP endpoint monitoring | Free (50 monitors) |
| **Sentry** | Error tracking & alerts | Free (under 10K errors/month) |
| **Prometheus + Grafana** | Custom metrics | Self-hosted (free) |
| **PostgreSQL Stats** | Database performance | Built-in (free) |
| **CloudWatch** | Infrastructure metrics | AWS free tier |

### Critical Metrics I Monitor

#### 1. Application Health (FastAPI)

```python
# health_check.py
from fastapi import FastAPI, HTTPException
from datetime import datetime
import psutil

app = FastAPI()

@app.get("/health")
async def health_check():
    """Comprehensive health check endpoint"""
    checks = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "checks": {}
    }
    
    # Check 1: Database connectivity
    try:
        db.execute("SELECT 1")
        checks["checks"]["database"] = "ok"
    except Exception as e:
        checks["status"] = "unhealthy"
        checks["checks"]["database"] = f"error: {str(e)}"
    
    # Check 2: Redis connectivity
    try:
        redis_client.ping()
        checks["checks"]["redis"] = "ok"
    except Exception as e:
        checks["status"] = "unhealthy"
        checks["checks"]["redis"] = f"error: {str(e)}"
    
    # Check 3: ML model loaded
    if model_registry.active_model is None:
        checks["status"] = "unhealthy"
        checks["checks"]["model"] = "not loaded"
    else:
        checks["checks"]["model"] = "ok"
    
    # Check 4: System resources
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_percent = psutil.virtual_memory().percent
    
    if cpu_percent > 90:
        checks["status"] = "degraded"
        checks["checks"]["cpu"] = f"high: {cpu_percent}%"
    else:
        checks["checks"]["cpu"] = f"ok: {cpu_percent}%"
    
    if memory_percent > 90:
        checks["status"] = "degraded"
        checks["checks"]["memory"] = f"high: {memory_percent}%"
    else:
        checks["checks"]["memory"] = f"ok: {memory_percent}%"
    
    # Check 5: Disk space
    disk_percent = psutil.disk_usage('/').percent
    if disk_percent > 85:
        checks["status"] = "warning"
        checks["checks"]["disk"] = f"high: {disk_percent}%"
    else:
        checks["checks"]["disk"] = f"ok: {disk_percent}%"
    
    # Return 200 for healthy/degraded, 503 for unhealthy
    status_code = 200 if checks["status"] in ["healthy", "degraded"] else 503
    return JSONResponse(content=checks, status_code=status_code)
```

**UptimeRobot setup:**
- Checks `/health` endpoint every 5 minutes
- Alerts via email + Discord webhook if status ‚â† 200
- Escalation: If down for >10 minutes, send SMS

#### 2. Model Performance Monitoring

**This is where most ML teams fail.** They monitor infrastructure but not the actual model.

```python
# model_monitor.py
import pandas as pd
from datetime import datetime, timedelta

class ModelPerformanceMonitor:
    def __init__(self, db_connection):
        self.db = db_connection
        self.alert_threshold = 0.68  # Alert if accuracy drops below 68%
    
    def log_prediction(self, game_id, prediction, confidence, features):
        """Log every prediction for later analysis"""
        self.db.execute("""
            INSERT INTO predictions_log 
            (game_id, prediction, confidence, features, timestamp)
            VALUES (%s, %s, %s, %s, %s)
        """, (game_id, prediction, confidence, json.dumps(features), datetime.now()))
    
    def log_actual_outcome(self, game_id, actual_result):
        """Log actual game outcome after game completes"""
        self.db.execute("""
            UPDATE predictions_log
            SET actual_result = %s, outcome_logged_at = %s
            WHERE game_id = %s
        """, (actual_result, datetime.now(), game_id))
    
    def check_accuracy_drift(self):
        """Check if model accuracy has degraded"""
        # Get last 100 predictions with actual outcomes
        query = """
            SELECT prediction, actual_result
            FROM predictions_log
            WHERE actual_result IS NOT NULL
            ORDER BY timestamp DESC
            LIMIT 100
        """
        results = pd.read_sql(query, self.db)
        
        if len(results) < 50:
            return  # Not enough data yet
        
        accuracy = (results['prediction'] == results['actual_result']).mean()
        
        if accuracy < self.alert_threshold:
            self.trigger_alert(
                "Model Accuracy Degradation",
                f"Accuracy dropped to {accuracy:.1%} (last 100 predictions)"
            )
    
    def check_feature_drift(self):
        """Detect if feature distributions have shifted"""
        # Get feature stats from last 7 days vs. previous 30 days
        recent_features = self.get_feature_stats(days=7)
        baseline_features = self.get_feature_stats(days=30, offset=7)
        
        for feature in recent_features.columns:
            recent_mean = recent_features[feature].mean()
            baseline_mean = baseline_features[feature].mean()
            
            # Alert if mean shifted by >20%
            percent_change = abs((recent_mean - baseline_mean) / baseline_mean)
            if percent_change > 0.2:
                self.trigger_alert(
                    "Feature Drift Detected",
                    f"{feature} shifted {percent_change:.1%}"
                )
```

**Automated daily checks:**
- Accuracy monitoring (runs every 6 hours)
- Feature drift detection (runs daily at 3 AM)
- Prediction volume anomalies (alert if below 50% of expected volume)

#### 3. API Latency Monitoring

```python
# middleware.py
from fastapi import Request
import time
from prometheus_client import Histogram

# Prometheus histogram for latency tracking
REQUEST_LATENCY = Histogram(
    'api_request_latency_seconds',
    'API request latency',
    ['method', 'endpoint', 'status']
)

@app.middleware("http")
async def add_latency_tracking(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    latency = time.time() - start_time
    
    # Log to Prometheus
    REQUEST_LATENCY.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).observe(latency)
    
    # Alert if p95 latency > 200ms
    if latency > 0.2:
        logger.warning(f"Slow request: {request.url.path} took {latency:.3f}s")
    
    # Add latency header for debugging
    response.headers["X-Response-Time"] = f"{latency:.3f}s"
    
    return response
```

**Grafana dashboard shows:**
- p50, p95, p99 latency by endpoint
- Request volume by endpoint
- Error rate by status code
- Slowest endpoints (for optimization)

### Alert Escalation Ladder

**Level 1: Warning (Email)**
- CPU > 80% for 5 minutes
- Memory > 80% for 5 minutes
- Latency p95 > 200ms for 10 minutes

**Level 2: Critical (Email + Discord)**
- Health check failing
- Accuracy < 68% for 6 hours
- Error rate > 5% for 10 minutes

**Level 3: Emergency (SMS + Phone Call)**
- Complete service outage
- Database unreachable
- Multiple critical alerts simultaneously

**Tool I use for SMS alerts:** Twilio ($0.0075 per SMS)

---

## Pillar 3: Zero-Downtime Deployments

**Rule #3: Never deploy on Friday. And always have a rollback plan.**

### Blue-Green Deployment for ML Models

**Problem:** Updating a model requires loading new weights into memory. During this time, the API is unresponsive.

**Solution:** Blue-green deployment

```python
# model_registry.py
import joblib
from pathlib import Path

class ModelRegistry:
    def __init__(self, model_dir: Path):
        self.model_dir = model_dir
        self.blue_model = self.load_latest_model()  # Currently serving
        self.green_model = None  # Standby for updates
        self.active = "blue"
    
    def load_latest_model(self):
        """Load most recent model from disk"""
        model_files = sorted(self.model_dir.glob("model_v*.pkl"))
        if not model_files:
            raise FileNotFoundError("No model files found")
        return joblib.load(model_files[-1])
    
    def update_model(self, new_model_path: Path):
        """Hot-swap model with zero downtime"""
        # Load new model into green slot
        self.green_model = joblib.load(new_model_path)
        
        # Validate model (smoke tests)
        if not self.validate_model(self.green_model):
            raise ValueError("Model failed validation")
        
        # Atomic swap
        if self.active == "blue":
            self.blue_model = self.green_model
            self.active = "green"
        else:
            self.green_model = self.blue_model
            self.active = "blue"
        
        logger.info(f"Model updated successfully. Active: {self.active}")
    
    def validate_model(self, model) -> bool:
        """Run smoke tests on new model"""
        try:
            # Test 1: Can it make predictions?
            test_features = self.get_test_features()
            predictions = model.predict(test_features)
            
            # Test 2: Predictions are in valid range?
            if not all(0 <= p <= 1 for p in predictions):
                return False
            
            # Test 3: Performance on validation set?
            val_accuracy = self.compute_validation_accuracy(model)
            if val_accuracy < 0.65:  # Minimum acceptable accuracy
                return False
            
            return True
        except Exception as e:
            logger.error(f"Model validation failed: {e}")
            return False
    
    def predict(self, features):
        """Always use active model for predictions"""
        active_model = self.blue_model if self.active == "blue" else self.green_model
        return active_model.predict(features)
```

**Deployment process:**
1. Train new model offline
2. Upload to server (doesn't affect live traffic)
3. Load into green slot + validate
4. Atomic swap (takes under 1 second)
5. Monitor for 10 minutes
6. If errors spike, rollback to previous model

**Downtime during deployment:** 0 seconds

### Database Migrations Without Downtime

**Problem:** Schema changes can lock tables, causing downtime.

**Solution:** Backward-compatible migrations

```python
# Example: Adding a new column without downtime

# Step 1: Add column as nullable (no data backfill yet)
# migration_001.py
def upgrade():
    op.add_column('predictions', sa.Column('confidence_score', sa.Float(), nullable=True))

# Deploy this migration (no downtime)

# Step 2: Backfill data in background (chunked)
# backfill_confidence.py
def backfill_confidence_scores():
    batch_size = 1000
    offset = 0
    
    while True:
        rows = db.execute("""
            SELECT id FROM predictions 
            WHERE confidence_score IS NULL
            LIMIT %s OFFSET %s
        """, (batch_size, offset)).fetchall()
        
        if not rows:
            break
        
        for row in rows:
            # Compute confidence score
            score = compute_confidence(row.id)
            db.execute("""
                UPDATE predictions
                SET confidence_score = %s
                WHERE id = %s
            """, (score, row.id))
        
        offset += batch_size
        time.sleep(1)  # Throttle to avoid overwhelming DB

# Step 3: Make column non-nullable (after backfill complete)
# migration_002.py
def upgrade():
    op.alter_column('predictions', 'confidence_score', nullable=False)
```

**Key principles:**
- Add columns as nullable first
- Backfill data in background
- Never drop columns in same release (gives time to rollback)

---

## Pillar 4: Incident Response (Fix Fast, Learn Faster)

**Rule #4: Incidents are inevitable. How you respond defines uptime.**

### My Incident Response Runbook

**Phase 1: Detection (Automated)**
- Monitoring systems detect anomaly
- Alert sent to on-call engineer (me)
- Incident automatically created in tracking system

**Phase 2: Triage (Within 5 minutes)**
```
1. Check status page: scardubu.dev/status
2. Verify monitoring alerts (not false positive)
3. Assess severity:
   - P0: Complete outage (all users affected)
   - P1: Partial outage (>50% users affected)
   - P2: Degraded performance (under 50% affected)
   - P3: Minor issue (no user impact)
4. Post incident update to status page
```

**Phase 3: Investigation (Parallel tracks)**

```python
# incident_investigation.py
class IncidentInvestigation:
    def run_diagnosis(self):
        """Run automated diagnostic checks"""
        
        # Check 1: Are servers responding?
        health_status = self.check_health_endpoints()
        
        # Check 2: Database connectivity?
        db_status = self.check_database()
        
        # Check 3: Redis availability?
        cache_status = self.check_redis()
        
        # Check 4: Recent deployments?
        recent_deploys = self.check_deployment_history(hours=2)
        
        # Check 5: Traffic spike?
        traffic_anomaly = self.detect_traffic_anomaly()
        
        # Check 6: Error logs?
        recent_errors = self.analyze_error_logs(minutes=30)
        
        # Generate diagnosis report
        return {
            "health": health_status,
            "database": db_status,
            "cache": cache_status,
            "recent_deploys": recent_deploys,
            "traffic_anomaly": traffic_anomaly,
            "errors": recent_errors
        }
```

**Phase 4: Resolution**
- Apply fix (often rollback to previous version)
- Verify fix in production
- Monitor for 30 minutes
- Update status page

**Phase 5: Post-Mortem (Within 48 hours)**

Template I use:

```markdown
# Incident Post-Mortem: [Brief Description]

**Date:** YYYY-MM-DD  
**Duration:** XX minutes  
**Severity:** PX  
**Users Affected:** XXX

## What Happened?
[Timeline of events]

## Root Cause
[Technical explanation]

## Why It Wasn't Caught Earlier
[Monitoring gaps]

## Resolution
[What fixed it]

## Action Items
- [ ] Improve monitoring for [X]
- [ ] Add automated test for [Y]
- [ ] Update runbook with [Z]

## Prevention
[How we'll prevent this in future]
```

### Real Incident Example: The 2:47 AM Database Crash

**What happened:**
- PostgreSQL primary ran out of disk space
- Standby couldn't promote (also low disk)
- API returned 500 errors for 23 minutes

**Root cause:**
- Log files weren't being rotated
- Accumulated 15GB of logs over 3 months
- No disk space monitoring

**Resolution:**
1. Manually cleaned up logs (freed 12GB)
2. Restarted PostgreSQL
3. API recovered immediately

**Prevention measures implemented:**
- Added disk space monitoring (alert at 85%)
- Configured log rotation (keep only 7 days)
- Set up automated cleanup cron job

**Cost of prevention:** $0 (just configuration)  
**Incidents prevented since:** 4 (based on alerts that triggered cleanup)

---

## Pillar 5: Cost Optimization (Reliability on a Budget)

**Rule #5: 99.9% uptime doesn't require enterprise pricing.**

### My Monthly Infrastructure Costs

| Component | Service | Cost | Uptime Contribution |
|-----------|---------|------|-------------------|
| **API Servers (2x)** | DigitalOcean Droplets | $48 | Redundancy prevents single-instance failures |
| **Database** | DigitalOcean Managed PostgreSQL | $15 | Auto-failover prevents DB outages |
| **Caching** | Redis Cloud | $5 | Reduces load on primary DB |
| **CDN/DDoS** | Cloudflare | $0 | Blocks attacks that would cause downtime |
| **Monitoring** | UptimeRobot + Sentry | $0 | Early detection prevents prolonged outages |
| **Backups** | DigitalOcean Snapshots | $2 | Disaster recovery |
| **Total** | ‚Äî | **$70/month** | ‚Äî |

**For comparison:**
- AWS equivalent: ~$300-400/month
- Enterprise monitoring (Datadog): $18+/user/month
- Managed Kubernetes: $150+/month

### Cost Optimization Strategies

**1. Use Managed Services for Critical Components**
- Don't manage PostgreSQL yourself ($15/month vs. 10 hours/month of ops work)
- Automated backups, failover, and scaling

**2. Self-Host Non-Critical Services**
- Prometheus + Grafana on cheapest droplet ($6/month)
- Saves $100+/month vs. Datadog/New Relic

**3. Aggressive Caching**
- Redis cache hits: 89%
- Saves database load ‚Üí can use cheaper DB tier

**4. Cloudflare Free Tier is Amazing**
- Would cost $200+/month on AWS CloudFront
- Includes DDoS protection that would be $1000+/month elsewhere

**5. Spot Instances for Batch Jobs**
- Model training on spot instances (70% cheaper)
- Hourly job monitoring doesn't need 24/7 instance

**Result:** 99.9% uptime for $70/month

---

## Lessons Learned from 12 Months of Production

### What Worked

‚úÖ **Redundancy everywhere:** Every single-point-of-failure eliminated saved me multiple outages  
‚úÖ **Proactive monitoring:** 80% of issues caught before users noticed  
‚úÖ **Automated health checks:** Detected problems at 3 AM when I was asleep  
‚úÖ **Post-mortems:** Each incident made system more resilient  
‚úÖ **Managed services:** $15/month PostgreSQL prevented countless self-inflicted wounds

### What Didn't Work

‚ùå **Manual deployments:** Caused 2 outages due to human error  
‚ùå **Weekend deploys:** Murphy's Law is real. Only deploy Mon-Thu.  
‚ùå **Ignoring disk space:** Caused my worst outage (23 minutes)  
‚ùå **Skipping smoke tests:** Deployed bad model once, caught in production  
‚ùå **Complex orchestration:** Kubernetes was overkill and increased failure modes

### Surprising Insights

üîç **Most outages are self-inflicted**  
- 60% of my incidents were caused by deployments/config changes
- Solution: Automated testing + gradual rollouts

üîç **Latency matters more than uptime**  
- Users tolerate 2 minutes of downtime
- But 5 seconds of slow responses? They leave permanently

üîç **Caching solves 80% of scale problems**  
- Redis cache hit rate: 89%
- Database queries reduced by 10x

---

## Your 99.9% Uptime Checklist

Use this as your starting point:

### Infrastructure
- [ ] At least 2 API instances with load balancer
- [ ] Database replication (primary + standby)
- [ ] Redis caching layer
- [ ] Cloudflare for DDoS protection
- [ ] Automated backups (tested monthly)

### Monitoring
- [ ] Uptime monitoring (UptimeRobot or similar)
- [ ] Error tracking (Sentry or similar)
- [ ] Performance metrics (latency, throughput)
- [ ] ML-specific: accuracy drift, feature drift
- [ ] Alert escalation (email ‚Üí Discord ‚Üí SMS)

### Deployment
- [ ] Blue-green deployment for ML models
- [ ] Automated smoke tests before rollout
- [ ] Database migrations are backward-compatible
- [ ] Rollback plan (1-click revert)
- [ ] Never deploy on Fridays

### Incident Response
- [ ] Incident response runbook
- [ ] Status page for user communication
- [ ] On-call rotation (even if it's just you)
- [ ] Post-mortem template
- [ ] Action items tracked to completion

### Cost Optimization
- [ ] Use managed services for critical components
- [ ] Self-host non-critical services
- [ ] Aggressive caching strategy
- [ ] Spot instances for batch jobs
- [ ] Regular cost audits

---

## Open-Source Tools I Use

Can't afford enterprise tools? Neither could I. Here's my free/cheap stack:

1. **[UptimeRobot](https://uptimerobot.com/)** - Free uptime monitoring (50 monitors)
2. **[Sentry](https://sentry.io/)** - Free error tracking (under 10K events/month)
3. **[Prometheus](https://prometheus.io/)** - Free metrics collection (self-hosted)
4. **[Grafana](https://grafana.com/)** - Free visualization dashboards
5. **[Cloudflare](https://www.cloudflare.com/)** - Free CDN + DDoS protection

**GitHub repo with my full monitoring setup:**  
[github.com/scardubu/ml-monitoring-stack](https://github.com/scardubu/ml-monitoring-stack) ‚≠ê Star if useful!

---

## Need Help with Your ML System?

Building production ML systems? I offer:

**MLOps Architecture Review** ($200/hour)
- Audit your current setup
- Identify reliability risks
- Actionable recommendations

**Full MLOps Consulting** (Custom pricing)
- Design high-availability architecture
- Implement monitoring + alerting
- On-call support during launch

**Mentorship for ML Engineers** ($100/month)
- Weekly 1-on-1 calls
- Code reviews
- Career guidance

[Schedule a call](https://scardubu.dev/#contact) | [Email me](mailto:scardubu@gmail.com)

---

## Your Turn

What's your ML system's uptime? What's your biggest production challenge?

Drop a comment below or reach out:
- üìß [scardubu@gmail.com](mailto:scardubu@gmail.com)
- üíº [LinkedIn](https://linkedin.com/in/oscardubu)
- üêô [GitHub](https://github.com/scardubu)

**Share this post if it helped:**  
[Tweet](https://twitter.com/intent/tweet?text=Achieving%2099.9%25%20Uptime%20for%20ML%20Systems%20by%20@scardubu&url=https://scardubu.dev/blog/mlops-playbook-999-uptime) | [LinkedIn](https://www.linkedin.com/sharing/share-offsite/?url=https://scardubu.dev/blog/mlops-playbook-999-uptime)

---

*Last updated: December 2025 | Reading time: 10 minutes*